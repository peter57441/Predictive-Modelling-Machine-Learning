---
title: "Opgavesæt 4"
output: html_document
date: "2023-03-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Opgave 1 a), b) og c)

Vi tager udgangspunkt i datasættet **biluheld.txt** for at prædiktere variablen *doed*. Nedenfor indlæsser vi data, standardiserer input-variablene og deler datasættet i et train/test-split med 30% i testdatasættet.


```{r}
library(keras)
library(tensorflow)
library(ggplot2)
library(readr)
```





```{r}



# Indlæser data
biluheld <- read_delim("biluheld.txt", delim = "\t", escape_double = FALSE, trim_ws = TRUE)

### opdeling i input og output ###
xBiluheld <- model.matrix(Doed~.,biluheld)[,-1]
yBiluheld <- biluheld$Doed

N <- length(yBiluheld)

### normalisering af input  ###
for (i in 1:ncol(xBiluheld)){
  xBiluheld[,i] <- (xBiluheld[,i]-mean(xBiluheld[,i]))/sd(xBiluheld[,i])
}


### opdeling i træning og test ###
set.seed(2023)
train <- sample(1:N,floor(0.7*N))
test <- (1:N)[-train]

```


### Opgave 1 d)

Vi fitter nu et neuralt netværk (NN) med 1 hidden layer og 6 hidden units, hvor sigmoid-funktionen benyttes som activation function i output-laget. Jeg lader 20% af testdata blive benyttet til validering. 


```{r}

##### modelbygning og fit
model <- keras_model_sequential() %>%
  layer_dense(units = 6, activation = "sigmoid", input_shape = ncol(xBiluheld)) %>%
  layer_dense(units = 1, activation = "sigmoid") %>%

  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)    
)

fit1 <- model %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )

```

### Opgave 1.e 

summary af modellen 

```{r}
summary(model)
```

Der er 85 parametre i alt. Hvilket passer med antal input (12) gange 6 hidden units plus 12 + 1 bias led i output layer dvs. $12 \cdot 6 + 12+1=85$.

### Opgave 1.f og 1.g

Plot af modellens fit MSE

```{r}

plot(fit1$metrics$loss[5:400], type="l")
lines(fit1$metrics$val_loss[5:400], col="red")

```


Jf. estimationsprocessen minimeres MSE ved højere epoch uden at validation loss stiger. Dermed ville vi ikke vurdere at der umiddelbart er tale om overfit, da validerings MSE ikke "stikker" af fra trænings MSE.

### Opgave 1 h

Vi gør samme procedure som før, men med 2 hidden layes og vurderer trænnings-loss og validation-loss.


```{r}


##### modelbygning og fit
model2 <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "sigmoid", input_shape = ncol(xBiluheld)) %>%
  layer_dense(units = 9, activation = "sigmoid") %>%
  layer_dense(units = 1, activation = "sigmoid") %>%

  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)    
)

fit2 <- model2 %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )

```


### summary af modellen 

```{r}
summary(model2)
```

### Plot af modellens fit MSE


```{r}


plot(fit2$metrics$loss[5:400], type="l")
lines(fit2$metrics$val_loss[5:400], col="red")

```


Validerings-loss stiger, hvorfor der nu umiddelbart vil være tale om overfit. Hvorfor det kan være relevant at tale om regularisering.


## Opgave 1 i

Vi inkluderer nu $L_2$-regularisering (Ridge) og dropout i hvert af lagene. Dvs. vi minimerer:

$$R(\theta) = \sum_{i=1}^N (y_i-f(x_i))^2+\lambda \sum_{\ell=1}^q \omega_{\ell}^2$$

Vi sætter $\lambda = 0.00001$ og dropout sandsynligheden sættes til $p = 0.05$. Og vi gentager samme procedure som tidligere derefter:


```{r}


##### modelbygning og fit

lambda <- 0.00001
dropout_rate <- 0.005

model3 <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "sigmoid", input_shape = ncol(xBiluheld), kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 9, activation = "sigmoid", kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 1, activation = "sigmoid") %>%

  compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)    
)

fit3 <- model3 %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )





```


### summary af modellen 

```{r}
summary(model3)
```

### Plot af modellens fit MSE


```{r}


plot(fit3$metrics$loss[5:400], type="l")
lines(fit3$metrics$val_loss[5:400], col="red")

```


Validerings-loss virker ikke længere til at stige og tænings-loss falder fortsat. Umiddelbart ville vi derfor ikke længere mene, at der er tale om overfit.


## Opgave 1 j


Vi vælger nu den bedste model med fit på hele træningsdatasættet (vi fjerne validation-split), og afprøver dens performance på testdatasættet. Vi udregner også fejlrate og AUC.

```{r}


fit1_no_val <- model %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    verbose = FALSE
  )

fit2_no_val <- model2 %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    verbose = FALSE
  )



fit3_no_val <- model3 %>%
  fit(
    x = xBiluheld[train,],
    y = yBiluheld[train],
    epochs = 400,
    batch_size = 64,
    verbose = FALSE
  )






```


Vi kigger nu på performance


```{r}
library(pROC)

# Model 1
pred.prob1 <- model %>% predict(xBiluheld[test,])
pred.value1<-1*(pred.prob1>0.5)
table(pred.value1,yBiluheld[test])
mean(pred.value1!=yBiluheld[test])
print(auc(yBiluheld[test]~pred.prob1,direction='<'))





```
```{r}

# Model 2
pred.prob2 <- model2 %>% predict(xBiluheld[test,])
pred.value2<-1*(pred.prob2>0.5)
table(pred.value2,yBiluheld[test])
mean(pred.value2!=yBiluheld[test])
print(auc(yBiluheld[test]~pred.prob2,direction='<'))


```

```{r}

# Model 3
pred.prob3 <- model3 %>% predict(xBiluheld[test,])
pred.value3<-1*(pred.prob3>0.5)
table(pred.value3,yBiluheld[test])
mean(pred.value3!=yBiluheld[test])
print(auc(yBiluheld[test]~pred.prob3,direction='<'))


```

Umiddelbart virker det til, at det er den første model der har den bedste performance. 


# Opgave 2 Neuralt netværk til regression


Vi kigger nu på syntetisk data til regression, hvor vi kører følgende kodestump for at generere to forskellige datasæt:


```{r}

N <- 5000

X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)

### opdeling i input og output ###
xData1 <- model.matrix(Y1~.,data1)[,-1]
yData1 <- data1$Y1

X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)

### opdeling i input og output ###
xData2 <- model.matrix(Y2~.,data2)[,-1]
yData2 <- data2$Y2

par(mfrow=c(1,2))
plot(Y1)
plot(Y2)

```

### Opgave 2 a

Det er sandsynligt, at det andet datasæt, data2, ville have mere gavn af at tilpasse et neuralt netværk end en standard OLS-model.

Årsagen til dette er, at forholdet mellem inputvariablerne (X21 og X22) og målvariablen (Y2) i data2 ser ud til at være mere komplekst og ikke-lineært sammenlignet med forholdet mellem inputvariablerne (X11 og X12) og målvariablen. (Y1) i data1. Specifikt genereres målvariablen i data2 ved hjælp af en ikke-lineær funktion af inputvariablerne, der involverer kvadratrødder, logaritmer og betingede udsagn (X21>0,5), hvilket tyder på, at der kan være interaktioner og ikke-lineariteter i dataene, som ikke er fanget af en lineær OLS-model.

I modsætning hertil genereres målvariablen i data1 ved hjælp af en lineær kombination af inputvariablerne (2*X11-3*X12) plus tilfældig støj, som effektivt kan modelleres ved hjælp af en lineær regression eller OLS-model.

Derfor kan et neuralt netværk være bedre egnet til at fange de komplekse, ikke-lineære relationer mellem input- og målvariablerne i data2, mens en lineær OLS-model kan være tilstrækkelig til at modellere det mere simple, lineære forhold i data1.

### Opgave 2 b

Vi fitter nu et NN med et hidden layer med 5 hidden units. Vi benytter 20% af testdata til validering. Vi benytter sigmoid som activation-function:


```{r}

### opdeling i træning og test ###

set.seed(2023)
train <- sample(1:N,floor(0.8*N))
test <- (1:N)[-train]

##### modelbygning og fit
model <- keras_model_sequential() %>%
  layer_dense(units = 5, activation = "sigmoid", input_shape = ncol(xData1)) %>%
  layer_dense(units = 1) %>%

compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)

fit1 <- model %>%
  fit(
    x = xData1[train,],
    y = yData1[train],
    epochs = 500,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )


fit2 <- model %>%
  fit(
    x = xData2[train,],
    y = yData2[train],
    epochs = 500,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )


```


```{r}

summary(model)

```


### Opgave 2 c

Vi udregner MSE for testdatasættet og sammenligner med `mean(epsilon[test]^2)`

```{r}


# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)


# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)



eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)

# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)

print(table_data)

```
Det virker til at den anden fittet på den anden model fitter markant bedre end den første. Samt virker den første model til at overfitte i højere grad end den anden model. 

Sammenligning af MSE med eps1- og eps2-værdierne kan give indsigt i ydeevnen af de neurale netværksmodeller på forskellige måder.

MSE måler den gennemsnitlige kvadratiske forskel mellem de forudsagte og faktiske værdier af målvariablen (i dette tilfælde Y1 eller Y2). Derfor indikerer en lavere MSE, at modellen er bedre til at forudsige målvariablen.

Værdierne eps1 og eps2 repræsenterer på den anden side variansen eller støjen i fejltermerne for den sande underliggende datagenereringsproces. Med andre ord repræsenterer de mængden af støj i de data, som det neurale netværk forsøger at lære og modellere.

Hvis det neurale netværk er i stand til at fange de underliggende mønstre og relationer i dataene, bør det være i stand til at producere lave MSE-værdier, selvom dataene er støjende. Men hvis det neurale netværk ikke er i stand til at fange de underliggende mønstre i dataene, kan det producere høje MSE-værdier, selvom dataene ikke er meget støjende.

Derfor kan sammenligning af MSE-værdierne med eps1- og eps2-værdierne hjælpe med at afgøre, om det neurale netværk over- eller underfitter dataene. Hvis MSE er høj sammenlignet med eps1 og eps2 værdierne, kan det tyde på, at det neurale netværk overfitter dataene og fanger støjen i dataene, snarere end de underliggende mønstre. Omvendt, hvis MSE er lav sammenlignet med eps1- og eps2-værdierne, kan det indikere, at det neurale netværk underfitter dataene og ikke fanger de underliggende mønstre i dataene.


### Opgave 2 d


Vi forsøger nu at gøre samme som før, men med et meget større og dybere NN - gerne med 20.451 parametre i alt. Samt benyttes $L_2$-regulariseing.  


```{r}


lambda <- 0.00001
dropout_rate <- 0.005


model <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", input_shape = ncol(xData1), kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 100, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 100, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
  layer_dropout(rate=dropout_rate) %>%
  layer_dense(units = 1, activation = "linear") %>%

compile(
  loss = 'mse',
  optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)


fit1 <- model %>%
  fit(
    x = xData1[train,],
    y = yData1[train],
    epochs = 500,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )


fit2 <- model %>%
  fit(
    x = xData2[train,],
    y = yData2[train],
    epochs = 500,
    batch_size = 64,
    validation_split = 0.2,
    verbose = FALSE
  )







```




```{r}

summary(model)

```

```{r}


# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)


# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)



eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)

###### alm lin regr  ####

lin.model <- lm(Y1~.,data=data1[train,])
koeff <- lin.model$coefficients
Xmat.test <- model.matrix(Y1~.,data=data1[test,])

predictions <- Xmat.test%*%koeff
MSE.lin1 <- mean((yData1[test]-predictions)^2)

lin.model <- lm(Y2~.,data=data2[train,])
koeff <- lin.model$coefficients
Xmat.test <- model.matrix(Y2~.,data=data2[test,])

predictions <- Xmat.test%*%koeff
MSE.lin2 <- mean((yData2[test]-predictions)^2)


# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2, MSE.lin1, MSE.lin2)

print(table_data)

```


### Opgave 2 e

Som forventet var MSE mindst for det andet datasæt ved NN og overfitter i lavere grad end før (og vurderes også til generelt at performe bedre). Den "simple" OLS model fungerer bedre i datasæt og er langt bedre end NN. Dog performer NN i datasæt 2 markant bedre end den "simple" OLS model, som også var forventet. Da data i sæt 2 ikke forekommer lineært, hvorfor OLS underperformer sammenlignet med NN. 












