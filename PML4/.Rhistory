labs(
x = "Annualized standard deviation (in percent)",
y = "Annualized expected return (in percent)",
title = "Efficient frontier(red) and 100 simulations(black)"
)
theme_bw()
hundsims <- hundsims + geom_point(
data = tibble(
mu1 = 12 * 100 * mu,
sd1 = 100 * sqrt(12) * sqrt(diag(Sigma))
),aes(y = mu1, x = sd1), size = 1,color='red')
hundsims
##Problem 8
c <- seq(0, 1, length.out = 100)
simulations <- 100
# function to generate simulated returns
simulate_returns <- function(periods = 200,
expected_returns = mu,
covariance_matrix = Sigma) {
MASS::mvrnorm(n = periods, expected_returns, covariance_matrix)
}
# function to compute efficient tangent portfolio
tangency_portfolio <- function(mu, Sigma, rf = 0) {
n <- 27
iota <- rep(1, n)
tang_weights <- solve(Sigma) %*% (mu - rf * iota) / as.numeric(t(iota) %*% solve(Sigma) %*% (mu - rf * iota))
return(tang_weights)
}
# set seed for reproducibility
set.seed(2023)
# generate hypothetical sample
sample_returns <- simulate_returns(periods = 200)
# compute sample mean and covariance matrix
sample_mu <- colMeans(sample_returns)
sample_sigma <- cov(sample_returns)
# compute plug-in estimates of the mean-variance frontier
sample_n <- ncol(sample_returns)
sample_iota <- rep(1, sample_n)
sample_mvp_weights <- solve(sample_sigma) %*% sample_iota
sample_mvp_weights <- sample_mvp_weights / sum(sample_mvp_weights)
sample_mu_bar <- 3 * t(sample_mvp_weights) %*% sample_mu
sample_C <- as.numeric(t(sample_iota) %*% solve(sample_sigma) %*% sample_iota)
sample_D <- as.numeric(t(sample_iota) %*% solve(sample_sigma) %*% sample_mu)
sample_E <- as.numeric(t(sample_mu) %*% solve(sample_sigma) %*% sample_mu)
sample_lambda_tilde <- as.numeric(2 * (sample_mu_bar - sample_D / sample_C) / (sample_E - sample_D^2 / sample_C))
sample_efp_weights <- sample_mvp_weights + sample_lambda_tilde / 2 * (solve(sample_sigma) %*% sample_mu - sample_D * sample_mvp_weights)
# set number of simulations
simulations <- 100
# initialize empty data frame to store results
results <- data.frame(matrix(ncol = 5, nrow = 100*length(c)))
colnames(results) <- c("c", "mu", "sigma", "sim", "SR")
# loop over simulations
for (i in 1:simulations) {
# generate simulated returns
sim_returns <- simulate_returns(periods = 200)
# compute sample mean and covariance matrix
sim_mu <- colMeans(sim_returns)
sim_sigma <- cov(sim_returns)
# compute plug-in estimates of the mean-variance frontier
sim_n <- ncol(sim_returns)
sim_iota <- rep(1, sim_n)
sim_mvp_weights <- solve(sim_sigma) %*% sim_iota
sim_mvp_weights <- sim_mvp_weights / sum(sim_mvp_weights)
sim_mu_bar <- 3 * t(sim_mvp_weights) %*% sim_mu
sim_C <- as.numeric(t(sim_iota) %*% solve(sim_sigma) %*% sim_iota)
sim_D <- as.numeric(t(sim_iota) %*% solve(sim_sigma) %*% sim_mu)
sim_E <- as.numeric(t(sim_mu) %*% solve(sim_sigma) %*% sim_mu)
sim_lambda_tilde <- as.numeric(2 * (sim_mu_bar - sim_D / sim_C) / (sim_E - sim_D^2 / sim_C))
sim_efp_weights <- sim_mvp_weights + sim_lambda_tilde / 2 * (solve(sim_sigma) %*% sim_mu - sim_D * sim_mvp_weights)
sim_SR <- sqrt(12) * t(sim_efp_weights) %*% sim_mu / sqrt(t(sim_efp_weights) %*% sim_sigma %*% sim_efp_weights)
# compute efficient tangent portfolio for simulated sample
sim_tang_weights <- tangency_portfolio(sim_mu, sim_sigma)
# compute Sharpe ratio for efficient portfolio using true parameters
SR <- sqrt(12) * t(sim_tang_weights) %*% mu / sqrt(t(sim_tang_weights) %*% Sigma %*% sim_tang_weights)
# compute efficient frontier for simulated sample
for (j in 1:length(c)) {
w <- (1 - c[j]) * sim_mvp_weights + c[j] * sim_efp_weights
SR_efficient <- sqrt(12) * t(w) %*% sim_mu / sqrt(t(w) %*% sim_sigma %*% w)
results[(i-1)*length(c) + j, ] <- c(c[j], 12*100*t(w)%*%sim_mu, 100*sqrt(12)*sqrt(t(w)%*%sim_sigma%*%w), i, SR_efficient)
}
}
# initialize empty vector to store annualized Sharpe ratio
portfolio_SR_vec <- numeric(simulations)
# loop over simulations
for (i in 1:simulations) {
# generate simulated returns
sim_returns <- simulate_returns(periods = 200)
# compute sample mean and covariance matrix
sim_mu <- colMeans(sim_returns)
sim_sigma <- cov(sim_returns)
# compute plug-in estimates of the mean-variance frontier
sim_n <- ncol(sim_returns)
sim_iota <- rep(1, sim_n)
sim_mvp_weights <- solve(sim_sigma) %*% sim_iota
sim_mvp_weights <- sim_mvp_weights / sum(sim_mvp_weights)
sim_mu_bar <- 3 * t(sim_mvp_weights) %*% sim_mu
sim_C <- as.numeric(t(sim_iota) %*% solve(sim_sigma) %*% sim_iota)
sim_D <- as.numeric(t(sim_iota) %*% solve(sim_sigma) %*% sim_mu)
sim_E <- as.numeric(t(sim_mu) %*% solve(sim_sigma) %*% sim_mu)
sim_lambda_tilde <- as.numeric(2 * (sim_mu_bar - sim_D / sim_C) / (sim_E - sim_D^2 / sim_C))
sim_efp_weights <- sim_mvp_weights + sim_lambda_tilde / 2 * (solve(sim_sigma) %*% sim_mu - sim_D * sim_mvp_weights)
sim_SR <- sqrt(12) * t(sim_efp_weights) %*% sim_mu / sqrt(t(sim_efp_weights) %*% sim_sigma %*% sim_efp_weights)
# compute efficient tangent portfolio for simulated sample
sim_tang_weights <- tangency_portfolio(sim_mu, sim_sigma)
# compute Sharpe ratio for efficient portfolio using true parameters
SR <- sqrt(12) * t(sim_tang_weights) %*% mu / sqrt(t(sim_tang_weights) %*% Sigma %*% sim_tang_weights)
# compute efficient frontier for simulated sample
for (j in 1:length(c)) {
w <- (1 - c[j]) * sim_mvp_weights + c[j] * sim_efp_weights
SR_efficient <- sqrt(12) * t(w) %*% sim_mu / sqrt(t(w) %*% sim_sigma %*% w)
results[(i-1)*length(c) + j, ] <- c(c[j], 12*100*t(w)%*%sim_mu, 100*sqrt(12)*sqrt(t(w)%*%sim_sigma%*%w), i, SR_efficient)
}
# compute annualized Sharpe ratio for efficient portfolio on efficient frontier
max_SR_idx <- which(results[, "SR"] == max(results[, "SR"]))
max_SR_w <- results[max_SR_idx, c("c", "mu", "sigma")]
max_SR_weights <- (1 - max_SR_w$c) * sim_mvp_weights + max_SR_w$c * sim_efp_weights
portfolio_SR_vec[i] <- sqrt(12) * t(max_SR_weights) %*% sim_mu / sqrt(t(max_SR_weights) %*% sim_sigma %*% max_SR_weights)
}
# print annualized Sharpe ratio for each simulation
print(portfolio_SR_vec)
max(portfolio_SR_vec)
#Problem 9
# set up colors
colors <- c("red", "green","blue")
# combine Sharpe ratios from portfolio_SR_vec and tang_SR_true into one vector
all_SR <- c(portfolio_SR_vec, tang_SR_true)
# find appropriate range for x-axis
xrange <- range(all_SR, na.rm = TRUE)
# create histogram with multiple colors
hist(portfolio_SR_vec, breaks = 20, xlim = xrange, main = "Histogram of Sharpe Ratios (200 periods)",
xlab = "Sharpe Ratio", col = colors[3], alpha = 0.5)
abline(v = tang_SR_true, lwd = 2, col = "red",lty="dashed")
abline(v = mean(portfolio_SR_vec), lwd = 2, col = "green", lty="dashed")
# add legend
legend("topleft", legend = c("Tangency Portfolio SR","Mean simulated SR","Simulated Sr" ), fill = colors, alpha = 0.2)
source("C:/Users/User/OneDrive/UNI/Advanced empirical finance/MA_1/Other groups/871290164701804.r", echo=TRUE)
# set number of simulations
simulations <- 100
results <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(results) <- c("c", "mu", "sigma", "sim")
# loop over simulations
for (i in 1:simulations) {
# generate simulated returns
sim_returns <- simulate_returns(periods = 200)
# compute sample mean and covariance matrix
sim_mu <- colMeans(sim_returns)
sim_sigma <- cov(sim_returns)
# Apply the mutual Funding Theorem
res2 <- compute_efficient_portfolio(sim_sigma, sim_mu)[3]
res2 <- res2[[1]]
# Craete the list for the table
res2$sim <- i ## Way to seperate the simulations
results <- rbind(results, res2)}
# convert to tibble
results <- as_tibble(results)
# plot efficient frontiers for each simulation and optimal tangency portfolio
hundsims <- ggplot(results, aes(x = sd, y = mu)) +
geom_point(size = 1,color='black') +
geom_point(
data = optt$data,
aes(x = sd, y = mu),
size = 2,
color = "red"
) +
labs(
x = "Annualized standard deviation (in percent)",
y = "Annualized expected return (in percent)",
title = "Efficient frontier(red) and 100 simulations(black)"
)
theme_bw()
hundsims <- hundsims + geom_point(
data = tibble(
mu1 = 12 * 100 * mu,
sd1 = 100 * sqrt(12) * sqrt(diag(Sigma))
),aes(y = mu1, x = sd1), size = 1,color='red')
hundsims
knitr::opts_chunk$set(echo = TRUE)
install.packages(c("tensorflow","reticulate"))
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(tensorflow)
library(ggplot2)
library(readr)
install_tensorflow()
install_keras()
knitr::opts_chunk$set(echo = TRUE)
N<-5000
X11<-rnorm(N)
X12<-rnorm(N)
epsilon1<-rnorm(N,mean=0,sd=0.2)
Y1<-2*X11-3*X12+epsilon1
data1<-data.frame(Y1,X11,X12)
X21<-rnorm(N)
X22<-rnorm(N)
epsilon2<-rnorm(N,mean=0,sd=0.5)
Y2<-(abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2<-data.frame(Y2,X21,X22)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
plot(Y1)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
plot(Y1)
plot(Y2)
library(keras)
library(tensorflow)
library(ggplot2)
library(readr)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
par(mfrow=c(1,2))
plot(Y1)
plot(Y2)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
par(mfrow=c(1,2))
plot(Y1)
plot(Y2)
### opdeling i træning og test ###
set.seed(2023)
train <- sample(1:N,floor(0.7*N))
test <- (1:N)[-train]
View(data1)
N <- 5000
X11 <- rnorm(N)
X12 <- rnorm(N)
epsilon1 <- rnorm(N,mean=0,sd=0.2)
Y1 <- 2*X11-3*X12+epsilon1
data1 <- data.frame(Y1,X11,X12)
### opdeling i input og output ###
xData1 <- model.matrix(Y1~.,data1)[,-1]
yData1 <- data1$Y1
X21 <- rnorm(N)
X22 <- rnorm(N)
epsilon2 <- rnorm(N,mean=0,sd=0.5)
Y2 <- (abs(2*X21+X22+2))^1.5+log(abs(X22)+1)*3*sqrt(X21*(X21>0.5))+epsilon2
data2 <- data.frame(Y2,X21,X22)
### opdeling i input og output ###
xData2 <- model.matrix(Y2~.,data2)[,-1]
yData2 <- data2$Y2
par(mfrow=c(1,2))
plot(Y1)
plot(Y2)
View(xData1)
View(xData2)
### opdeling i træning og test ###
set.seed(2023)
train <- sample(1:N,floor(0.8*N))
test <- (1:N)[-train]
##### modelbygning og fit
model1 <- keras_model_sequential() %>%
layer_dense(units = 5, activation = "sigmoid", input_shape = ncol(xData1)) %>%
layer_dense(units = 1) %>%
compile(
loss = 'mse',
optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)
fit1 <- model1 %>%
fit(
x = xData1[train,],
y = yData1[train],
epochs = 500,
batch_size = 64,
validation_split = 0.2,
verbose = FALSE
)
### opdeling i træning og test ###
set.seed(2023)
train <- sample(1:N,floor(0.8*N))
test <- (1:N)[-train]
##### modelbygning og fit
model <- keras_model_sequential() %>%
layer_dense(units = 5, activation = "sigmoid", input_shape = ncol(xData1)) %>%
layer_dense(units = 1) %>%
compile(
loss = 'mse',
optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)
fit1 <- model %>%
fit(
x = xData1[train,],
y = yData1[train],
epochs = 500,
batch_size = 64,
validation_split = 0.2,
verbose = FALSE
)
fit2 <- model %>%
fit(
x = xData2[train,],
y = yData2[train],
epochs = 500,
batch_size = 64,
validation_split = 0.2,
verbose = FALSE
)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
MSE.NN1
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
MSE.NN1
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
MSE.NN2
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
MSE.NN1
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
MSE.NN2
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)
table_data
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)
print(table_data)
summary(model)
model <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu", input_shape = ncol(xData1)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 1, activation = "linear") %>%
compile(
loss = 'mse',
optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)
model <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu", input_shape = ncol(xData1)) %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 100, activation = "relu") %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dense(units = 1, activation = "linear") %>%
compile(
loss = 'mse',
optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)
lambda <- 0.00001
dropout_rate <- 0.005
model <- keras_model_sequential() %>%
layer_dense(units = 50, activation = "relu", input_shape = ncol(xData1), kernel_regularizer = regularizer_l2(lambda)) %>%
layer_dropout(rate=dropout_rate) %>%
layer_dense(units = 100, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
layer_dropout(rate=dropout_rate) %>%
layer_dense(units = 100, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
layer_dropout(rate=dropout_rate) %>%
layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(lambda)) %>%
layer_dropout(rate=dropout_rate) %>%
layer_dense(units = 1, activation = "linear") %>%
compile(
loss = 'mse',
optimizer = optimizer_rmsprop(learning_rate=0.001, rho=0.9)
)
fit1 <- model %>%
fit(
x = xData1[train,],
y = yData1[train],
epochs = 500,
batch_size = 64,
validation_split = 0.2,
verbose = FALSE
)
fit2 <- model %>%
fit(
x = xData2[train,],
y = yData2[train],
epochs = 500,
batch_size = 64,
validation_split = 0.2,
verbose = FALSE
)
summary(model)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)
print(table_data)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
###### alm lin regr  ####
lin.model <- lm(Y1~.,data=data1[train,])
koeff <- lin.model$coefficients
Xmat.test <- model.matrix(Y1~.,data=data1[test,])
predictions <- Xmat.test%*%koeff
MSE.lin <- mean((yData1[test]-predictions)^2)
MSE.lin
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2)
print(table_data)
# Model 1
test_predictions1 <- model %>% predict(xData1[test,])
MSE.NN1 <- mean((test_predictions1-yData1[test])^2)
# Model 2
test_predictions2 <- model %>% predict(xData2[test,])
MSE.NN2 <- mean((test_predictions2-yData2[test])^2)
eps1 <- mean(epsilon1[test]^2)
eps2 <- mean(epsilon2[test]^2)
###### alm lin regr  ####
lin.model <- lm(Y1~.,data=data1[train,])
koeff <- lin.model$coefficients
Xmat.test <- model.matrix(Y1~.,data=data1[test,])
predictions <- Xmat.test%*%koeff
MSE.lin1 <- mean((yData1[test]-predictions)^2)
lin.model <- lm(Y2~.,data=data2[train,])
koeff <- lin.model$coefficients
Xmat.test <- model.matrix(Y2~.,data=data2[test,])
predictions <- Xmat.test%*%koeff
MSE.lin2 <- mean((yData2[test]-predictions)^2)
# Laver tabel
table_data <- data.frame(MSE.NN1, MSE.NN2, eps1, eps2, MSE.lin1, MSE.lin2)
print(table_data)
